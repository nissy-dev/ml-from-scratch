# Clustering

目次

階層クラスタリング  
* クラスタ数を決めなくて良い
* [k-NN](#k-NN)

非階層クラスタリング
* 最初にクラスタ数を決める
* [k-means](#k-means)

手法は色々あるからsklearnで確認すると良い。  
* [https://scikit-learn.org/stable/modules/classes.html#classes](https://scikit-learn.org/stable/modules/classes.html#classes)


## k-means
日本語では、k平均法と呼ばれる

### 実装
```
$ python k_means/kmeans.py
```

### 理論

アルゴリズムは非常に単純！

1. 各クラスの重心の初期値を決定する
2. 各重心に近い点を新しいクラスに帰属
3. 重心の再計算
4. 重心の差分が閾値より大きければ、2,3を繰り返す

### 特徴

* **与えられたデータがどのようなデータ集合に分類されるのかを推論する手法(分類)**
* 教師なし学習
* 初期値依存性が問題になることがある  
→ k-means++などがある
* **簡単だが、線形分離可能なデータしか分けることができない**  
→ EM法やkernel k-means法などがある

> 次のような運用が安全だと言えます。
> * ループはできる限り収束するまで回す
> * ある k (クラスタ数) においてはランダムで初期値を変えて複数回実行し、クラスタ内距離平均のような KPI で最適なクラスの分け方をみつける
> * いくつかの k について上記を試し、おさまりの良さそうな k を決定する

## k-NN

日本語では、k近傍法と呼ばれる

### 実装
```
$ python k_nn/knn.py
```

### 理論

距離が近い順にk個集めてそれらの多数決から目的とする値を求める手法。

1. 与えられたデータに対して、ユークリッド距離を算出
2. 近い順からk個集める
3. それらのラベルから多数決で予想値を決定

事前学習できることはないので、fit関数もそのような実装になっている。  
**kの値は、同数票を防ぐため奇数が選択される。**

### 特徴

* **データの特徴から、あらかじめ知られているクラスのどこに分類するのかを推論する(識別)**
* 教師あり学習
* アルゴリズムが簡単
* kの値の決定方法が課題  
→ 小さいと過学習気味になり、大きいと外れ値を引きすぎる
